{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "eacac3a5",
      "metadata": {
        "id": "eacac3a5"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "### Python Imports\n",
        "   * [Link to package import](#import_packages)\n",
        "   * [Link to training data import](#import_training_data)\n",
        "   \n",
        "### Data Formatting\n",
        "* [Article List Formating](#generate_python_list)\n",
        "* [Article Retrieval Code](#retrieve_articles)\n",
        "\n",
        "### Article Retrieval (TF-IDF)\n",
        "* [Data Preprocessing](#preprocess_data)\n",
        "* [Document Retrieval Benchmarking](#document_retrieval_benchmarking)\n",
        "* [Pretrained Model](#import_pretrained_model)\n",
        "\n",
        "### Answer Retrieval (BERT)\n",
        "* [BERT Implementation](#BERT_training)\n",
        "* [BERT Looping Helper Functions](#looping_helpers)\n",
        "* [BERT Standard Benchmarking](#standard_benchmarking)\n",
        "* [BERT Benchmarking with Looping](#benchmarking_BERT_with_loop)\n",
        "* [BERT Benchmarking Troubleshooting](#BERT_analysis)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install pandas\n",
        "!pip install nltk\n",
        "!pip install torch\n",
        "!pip install spacy\n",
        "!pip install sklearn\n",
        "!pip install scipy"
      ],
      "metadata": {
        "id": "moAFR_VDLS-a"
      },
      "id": "moAFR_VDLS-a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0ccdbdac",
      "metadata": {
        "id": "0ccdbdac"
      },
      "source": [
        "## Import Python Packages <a id='import_packages'></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "a0707258",
      "metadata": {
        "id": "a0707258"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from scipy.sparse import csr_matrix\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import transformers\n",
        "transformers.logging.set_verbosity_error()\n",
        "# from transformers import BertTokenizer, AutoTokenizer, BertForQuestionAnswering, BertTokenizerFast, BertConfig, DistilBertForQuestionAnswering, DistilBertTokenizerFast\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from transformers import BertTokenizer,AutoTokenizer,BertForQuestionAnswering\n",
        "from torch.optim import AdamW\n",
        "import random\n",
        "import re\n",
        "import spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "452ca227",
      "metadata": {
        "id": "452ca227"
      },
      "source": [
        "## Import Training Sets <a id='import_training_data'></a>\n",
        "### PLEASE CHANGE THE FILE PATH OF WHERE THE TRAINING AND TEST DATA FILES ARE LOCATED!!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "1300cdd5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "1300cdd5",
        "outputId": "b81d9d39-e861-449c-9ad4-cb95dd3421c1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "        Unnamed: 0      topic  \\\n",
              "0                0    Beyonce   \n",
              "1                1    Beyonce   \n",
              "2                2    Beyonce   \n",
              "3                3    Beyonce   \n",
              "4                4    Beyonce   \n",
              "...            ...        ...   \n",
              "130046      130046  Kathmandu   \n",
              "130047      130047  Kathmandu   \n",
              "130048      130048  Kathmandu   \n",
              "130049      130049  Kathmandu   \n",
              "130050      130050  Kathmandu   \n",
              "\n",
              "                                                 question  \\\n",
              "0                When did Beyonce start becoming popular?   \n",
              "1       What areas did Beyonce compete in when she was...   \n",
              "2       When did Beyonce leave Destiny's Child and bec...   \n",
              "3           In what city and state did Beyonce  grow up?    \n",
              "4              In which decade did Beyonce become famous?   \n",
              "...                                                   ...   \n",
              "130046  In what US state did Kathmandu first establish...   \n",
              "130047               What was Yangon previously known as?   \n",
              "130048  With what Belorussian city does Kathmandu have...   \n",
              "130049  In what year did Kathmandu create its initial ...   \n",
              "130050                      What is KMC an initialism of?   \n",
              "\n",
              "                             answer  answer_start               question_id  \\\n",
              "0                 in the late 1990s         269.0  56be85543aeaaa14008c9063   \n",
              "1               singing and dancing         207.0  56be85543aeaaa14008c9065   \n",
              "2                              2003         526.0  56be85543aeaaa14008c9066   \n",
              "3                    Houston, Texas         166.0  56bf6b0f3aeaaa14008c9601   \n",
              "4                        late 1990s         276.0  56bf6b0f3aeaaa14008c9602   \n",
              "...                             ...           ...                       ...   \n",
              "130046                       Oregon         229.0  5735d259012e2f140011a09d   \n",
              "130047                      Rangoon         414.0  5735d259012e2f140011a09e   \n",
              "130048                        Minsk         476.0  5735d259012e2f140011a09f   \n",
              "130049                         1975         199.0  5735d259012e2f140011a0a0   \n",
              "130050  Kathmandu Metropolitan City           0.0  5735d259012e2f140011a0a1   \n",
              "\n",
              "                                           answer_context  \n",
              "0       Beyonce Giselle Knowles-Carter (/bi:'janseI/ b...  \n",
              "1       Beyonce Giselle Knowles-Carter (/bi:'janseI/ b...  \n",
              "2       Beyonce Giselle Knowles-Carter (/bi:'janseI/ b...  \n",
              "3       Beyonce Giselle Knowles-Carter (/bi:'janseI/ b...  \n",
              "4       Beyonce Giselle Knowles-Carter (/bi:'janseI/ b...  \n",
              "...                                                   ...  \n",
              "130046  Kathmandu Metropolitan City (KMC), in order to...  \n",
              "130047  Kathmandu Metropolitan City (KMC), in order to...  \n",
              "130048  Kathmandu Metropolitan City (KMC), in order to...  \n",
              "130049  Kathmandu Metropolitan City (KMC), in order to...  \n",
              "130050  Kathmandu Metropolitan City (KMC), in order to...  \n",
              "\n",
              "[86820 rows x 7 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0905e257-19f0-4818-8a86-90f2149273c2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>topic</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>question_id</th>\n",
              "      <th>answer_context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Beyonce</td>\n",
              "      <td>When did Beyonce start becoming popular?</td>\n",
              "      <td>in the late 1990s</td>\n",
              "      <td>269.0</td>\n",
              "      <td>56be85543aeaaa14008c9063</td>\n",
              "      <td>Beyonce Giselle Knowles-Carter (/bi:'janseI/ b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Beyonce</td>\n",
              "      <td>What areas did Beyonce compete in when she was...</td>\n",
              "      <td>singing and dancing</td>\n",
              "      <td>207.0</td>\n",
              "      <td>56be85543aeaaa14008c9065</td>\n",
              "      <td>Beyonce Giselle Knowles-Carter (/bi:'janseI/ b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Beyonce</td>\n",
              "      <td>When did Beyonce leave Destiny's Child and bec...</td>\n",
              "      <td>2003</td>\n",
              "      <td>526.0</td>\n",
              "      <td>56be85543aeaaa14008c9066</td>\n",
              "      <td>Beyonce Giselle Knowles-Carter (/bi:'janseI/ b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Beyonce</td>\n",
              "      <td>In what city and state did Beyonce  grow up?</td>\n",
              "      <td>Houston, Texas</td>\n",
              "      <td>166.0</td>\n",
              "      <td>56bf6b0f3aeaaa14008c9601</td>\n",
              "      <td>Beyonce Giselle Knowles-Carter (/bi:'janseI/ b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Beyonce</td>\n",
              "      <td>In which decade did Beyonce become famous?</td>\n",
              "      <td>late 1990s</td>\n",
              "      <td>276.0</td>\n",
              "      <td>56bf6b0f3aeaaa14008c9602</td>\n",
              "      <td>Beyonce Giselle Knowles-Carter (/bi:'janseI/ b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130046</th>\n",
              "      <td>130046</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>In what US state did Kathmandu first establish...</td>\n",
              "      <td>Oregon</td>\n",
              "      <td>229.0</td>\n",
              "      <td>5735d259012e2f140011a09d</td>\n",
              "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130047</th>\n",
              "      <td>130047</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>What was Yangon previously known as?</td>\n",
              "      <td>Rangoon</td>\n",
              "      <td>414.0</td>\n",
              "      <td>5735d259012e2f140011a09e</td>\n",
              "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130048</th>\n",
              "      <td>130048</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>With what Belorussian city does Kathmandu have...</td>\n",
              "      <td>Minsk</td>\n",
              "      <td>476.0</td>\n",
              "      <td>5735d259012e2f140011a09f</td>\n",
              "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130049</th>\n",
              "      <td>130049</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>In what year did Kathmandu create its initial ...</td>\n",
              "      <td>1975</td>\n",
              "      <td>199.0</td>\n",
              "      <td>5735d259012e2f140011a0a0</td>\n",
              "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>130050</th>\n",
              "      <td>130050</td>\n",
              "      <td>Kathmandu</td>\n",
              "      <td>What is KMC an initialism of?</td>\n",
              "      <td>Kathmandu Metropolitan City</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5735d259012e2f140011a0a1</td>\n",
              "      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>86820 rows Ã— 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0905e257-19f0-4818-8a86-90f2149273c2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0905e257-19f0-4818-8a86-90f2149273c2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0905e257-19f0-4818-8a86-90f2149273c2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#flag for displaying the dataframes\n",
        "display_dataframe = True\n",
        "\n",
        "#flag for using the test data\n",
        "useTestData = False\n",
        "\n",
        "# Import the csv file containing the BERT test and training data\n",
        "training_BERT_filename = \"/content/squad_training_dataset_BERT.csv\"\n",
        "test_BERT_filename = \"/content/squad_test_training_dataset_BERT.csv\"\n",
        "training_BERT_dataset_df = pd.read_csv(training_BERT_filename)\n",
        "test_BERT_dataset_df = pd.read_csv(test_BERT_filename)\n",
        "\n",
        "# drop non-answers from the dataset\n",
        "training_BERT_dataset_df = training_BERT_dataset_df.dropna()\n",
        "test_BERT_dataset_df = test_BERT_dataset_df.dropna()\n",
        "\n",
        "# display the training set dataframe\n",
        "if display_dataframe:\n",
        "  if(useTestData):\n",
        "      display(test_BERT_dataset_df)\n",
        "  else:\n",
        "      display(training_BERT_dataset_df)\n",
        "\n",
        "# create training lists\n",
        "if(useTestData):\n",
        "    questions_BERT = test_BERT_dataset_df['question'].to_list()\n",
        "    answers_BERT = test_BERT_dataset_df['answer'].to_list()\n",
        "    topics_BERT = test_BERT_dataset_df['topic'].to_list()\n",
        "    answers_start = test_BERT_dataset_df['answer_start'].to_list()\n",
        "else:\n",
        "    questions_BERT = training_BERT_dataset_df['question'].to_list()\n",
        "    answers_BERT = training_BERT_dataset_df['answer'].to_list()\n",
        "    topics_BERT = training_BERT_dataset_df['topic'].to_list()\n",
        "    answers_start = training_BERT_dataset_df['answer_start'].to_list()\n",
        "\n",
        "# Import the csv files containing test and training data for article to questions\n",
        "topic_to_article_BERT_filename = \"/content/topics_to_articles_BERT.csv\"\n",
        "test_topic_to_article_BERT_filename = \"/content/topics_to_articles_BERT.csv\"\n",
        "topics_articles_BERT_df = pd.read_csv(topic_to_article_BERT_filename)\n",
        "topics_articles_test_BERT_df = pd.read_csv(test_topic_to_article_BERT_filename)\n",
        "\n",
        "if(useTestData):\n",
        "    #get the unique topic names from the dataframe\n",
        "    topic_strings_BERT = topics_articles_test_BERT_df.columns.to_list()\n",
        "    #get the correct label for the questions\n",
        "    question_topic_BERT = test_BERT_dataset_df['topic'].to_list()\n",
        "    #get the correct answers for questions\n",
        "    question_answers_BERT = test_BERT_dataset_df['answer'].to_list()\n",
        "else:\n",
        "    #get the unique topic names from the dataframe\n",
        "    topic_strings_BERT = topics_articles_BERT_df.columns.to_list()\n",
        "    #get the correct label for the questions\n",
        "    question_topic_BERT = training_BERT_dataset_df['topic'].to_list()\n",
        "    #get the correct answers for questions\n",
        "    question_answers_BERT = training_BERT_dataset_df['answer'].to_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a29f24ef",
      "metadata": {
        "id": "a29f24ef"
      },
      "source": [
        "## Generate List of Articles <a id='generate_python_list'></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2cea8d8b",
      "metadata": {
        "id": "2cea8d8b"
      },
      "outputs": [],
      "source": [
        "def Get_Article(df,topic):\n",
        "    article = df[topic].to_list()[0]\n",
        "    \n",
        "    return article\n",
        "\n",
        "articles_BERT = []\n",
        "for topic in topic_strings_BERT:\n",
        "    articles_BERT.append(Get_Article(topics_articles_BERT_df,topic))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bc295332",
      "metadata": {
        "id": "bc295332"
      },
      "outputs": [],
      "source": [
        "def segment_documents(docs, max_doc_length=500):\n",
        "    # List containing full and segmented docs\n",
        "    segmented_docs = []\n",
        "\n",
        "    for doc in docs:\n",
        "        # Split document by spaces to obtain a word count that roughly approximates the token count\n",
        "        split_to_words = doc.split(\" \")\n",
        "\n",
        "        # If the document is longer than our maximum length, split it up into smaller segments and add them to the list \n",
        "        if len(split_to_words) > max_doc_length:\n",
        "            for doc_segment in range(0, len(split_to_words), max_doc_length):\n",
        "                segmented_docs.append(\" \".join(split_to_words[doc_segment:doc_segment + max_doc_length]))\n",
        "\n",
        "        # If the document is shorter than our maximum length, add it to the list\n",
        "        else:\n",
        "            segmented_docs.append(doc)\n",
        "\n",
        "    return segmented_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e992d41",
      "metadata": {
        "id": "2e992d41"
      },
      "source": [
        "## Article Retrieval Code <a id='retrieve_articles'></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "985666fd",
      "metadata": {
        "id": "985666fd"
      },
      "outputs": [],
      "source": [
        "#make pre-processing functions\n",
        "def convert_lower_case(data):\n",
        "    return str(np.char.lower(data))\n",
        "\n",
        "def remove_punctuation(data):\n",
        "    new_data = \"\"\n",
        "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
        "    for i in symbols:\n",
        "        new_data = np.char.replace(data, i, ' ')\n",
        "        \n",
        "    return(str(new_data))\n",
        "def remove_apostrophe(data):\n",
        "    return str(np.char.replace(data, \"'\", \"\"))\n",
        "\n",
        "def remove_single_characters(data):\n",
        "    new_text = \"\"\n",
        "    \n",
        "    word_list = nltk.word_tokenize(data)\n",
        "    \n",
        "    for w in word_list:\n",
        "        if len(w) > 1:\n",
        "            new_text = new_text + \" \" + w\n",
        "    \n",
        "    return new_text\n",
        "\n",
        "def Lemmatize(data):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    \n",
        "    word_list = nltk.word_tokenize(data)\n",
        "    \n",
        "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
        "    \n",
        "    return lemmatized_output\n",
        "def Stemming(data):\n",
        "    ps = PorterStemmer()\n",
        "    \n",
        "    word_list = nltk.word_tokenize(data)\n",
        "    \n",
        "    stem_output = ' '.join([ps.stem(w) for w in word_list])\n",
        "    \n",
        "    return stem_output\n",
        "\n",
        "#create a function to preprocess the data\n",
        "def Pre_Process_Data(data):\n",
        "    new_data = remove_punctuation(data)\n",
        "    return new_data\n",
        "\n",
        "def Retrieve_Article(query, docs, k=5):\n",
        "    #pre-process the query\n",
        "    query = Pre_Process_Data(query)\n",
        "    \n",
        "    query_words = re.split('\\s+', query)\n",
        "    num_cols = len(query_words)\n",
        "    \n",
        "    # Initialize a vectorizer that removes English stop words\n",
        "    vectorizer = TfidfVectorizer(analyzer=\"word\", stop_words='english',sublinear_tf=True,use_idf=True)\n",
        "    \n",
        "    # Create a corpus of query and documents and convert to TFIDF vectors\n",
        "    query_and_docs = [query] + docs\n",
        "    matrix = vectorizer.fit_transform(query_and_docs)\n",
        "    \n",
        "    #apply SVD to the TF-IDF vectorized matrix\n",
        "    svd = TruncatedSVD(n_components=num_cols+250,n_iter=1,random_state=42)\n",
        "    \n",
        "    #fit and transform the SVD model\n",
        "    matrix_new = svd.fit_transform(matrix)\n",
        "    matrix_new = csr_matrix(matrix_new)\n",
        "\n",
        "    # Holds our cosine similarity scores\n",
        "    scores = []\n",
        "\n",
        "    # The first vector is our query text, so compute the similarity of our query against all document vectors\n",
        "    for i in range(1, len(query_and_docs)):\n",
        "        scores.append(cosine_similarity(matrix_new[0], matrix_new[i])[0][0])\n",
        "\n",
        "    # Sort list of scores and return the top k highest scoring documents\n",
        "    sorted_list = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
        "    top_doc_indices = [x[0] for x in sorted_list[:k]]\n",
        "    top_docs = [docs[x] for x in top_doc_indices]\n",
        "\n",
        "    return top_docs, top_doc_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bfe1eb7",
      "metadata": {
        "id": "7bfe1eb7"
      },
      "source": [
        "## Benchmark Article/Document Retrieval <a id='document_retrieval_benchmarking'></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "5b8292f9",
      "metadata": {
        "id": "5b8292f9"
      },
      "outputs": [],
      "source": [
        "#flag for running the document/article retrival benchmarking test\n",
        "RunDocRetrBenchmarking = False\n",
        "\n",
        "#conduct document retrieval for each question in the dataset\n",
        "def Benchmark_DocRetrieval(num_articles,num_samples,questions,question_topic,articles,articles_true,topic_strings,RandQs=False):\n",
        "    import random\n",
        "    \n",
        "    if(RandQs):\n",
        "        random_indices = list(random.sample(range(0, len(questions)), num_samples))\n",
        "\n",
        "        sample_questions = []\n",
        "        for idx in random_indices:\n",
        "            sample_questions.append(questions[idx])\n",
        "    else:\n",
        "        #create a sample of questions\n",
        "        sample_questions = questions[0:10]\n",
        "    \n",
        "    total = len(sample_questions)\n",
        "    correct = 0\n",
        "    tracker = 1\n",
        "    for question in sample_questions:\n",
        "        #create a tracker\n",
        "#         print(question)\n",
        "        print(\"Question #%d/%d\" %(tracker,total))\n",
        "\n",
        "        #get the true label for the question\n",
        "        true_question_label = question_topic[questions.index(question)]\n",
        "\n",
        "        #run the doc retriever on the current question \n",
        "        top_articles, predicted_articles_indices = Retrieve_Article(question,articles,k=num_articles)\n",
        "\n",
        "#         predicted_articles = []\n",
        "#         for idx in predicted_articles_indices:\n",
        "#             predicted_articles.append(articles_true[idx])\n",
        "        \n",
        "        #iterate over all predicted articles and check if the prediction is correct\n",
        "        for prediction in top_articles:    \n",
        "            #get the true topic of the predicted article\n",
        "            true_article_label = topic_strings[articles_true.index(prediction)]\n",
        "\n",
        "            #this will handle if the correct article is even chosen\n",
        "            if(true_question_label==true_article_label):\n",
        "                correct += 1\n",
        "\n",
        "        tracker += 1\n",
        "\n",
        "    return correct/total\n",
        "\n",
        "if(RunDocRetrBenchmarking):\n",
        "    num_top_articles = [1,3,5,10]\n",
        "    doc_accuracy_results = []\n",
        "    num_rand_samples = 10\n",
        "    for i in range(len(num_top_articles)):\n",
        "        print(\"Retrieving the top %d articles for each question...\" % num_top_articles[i])\n",
        "        retrieval_accuracy = Benchmark_DocRetrieval(num_top_articles[i],num_rand_samples,questions_BERT,question_topic_BERT,articles_BERT,articles_BERT,topic_strings_BERT,RandQs=True)\n",
        "        doc_accuracy_results.append(retrieval_accuracy)\n",
        "        print(\"k=%d Article Retrieval Accuracy: %.3f\" % (num_top_articles[i],retrieval_accuracy))\n",
        "\n",
        "    #plot the accuracy\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    sns.set()\n",
        "\n",
        "    sns.set_theme(font_scale=1)\n",
        "    accPlot = sns.lineplot(x=num_top_articles, y=doc_accuracy_results)\n",
        "    accPlot.set_title(\"Custom Article Retrieval Model Performance\\n 1000 Random Question Samples per iteration\")\n",
        "    accPlot.set(xlabel = \"Number of Articles Retrieved\", ylabel = \"Accuracy\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set-up for GPU use"
      ],
      "metadata": {
        "id": "Vl9kXJiyL7m0"
      },
      "id": "Vl9kXJiyL7m0"
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxqaObbnL_M0",
        "outputId": "bc84fcfe-b21f-459b-b8be-bab54fae4977"
      },
      "id": "hxqaObbnL_M0",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04cb5a62",
      "metadata": {
        "id": "04cb5a62"
      },
      "source": [
        "## Import the PreTrained or the Custom-Trained Model <a id='import_pretrained_model'></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "2b9bac6a",
      "metadata": {
        "id": "2b9bac6a"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "transformers.logging.set_verbosity_error()\n",
        "from transformers import BertTokenizer, AutoTokenizer, BertForQuestionAnswering, BertTokenizerFast, BertConfig, DistilBertForQuestionAnswering, DistilBertTokenizerFast\n",
        "\n",
        "#if this flag is true, you can run the code using the test data\n",
        "useCustomModel = False\n",
        "\n",
        "if(useCustomModel):\n",
        "    #this will only work if you have loaded the custom model into your Google Colaboratoty environment!\n",
        "    model_path = '/content/Custom_BERT_model_5'\n",
        "    model = BertForQuestionAnswering.from_pretrained(model_path).to(device)\n",
        "    tokenizer = BertTokenizer.from_pretrained(model_path)\n",
        "else:\n",
        "    modelname = 'deepset/bert-base-cased-squad2'\n",
        "    # modelname = 'deepset/bert-large-uncased-whole-word-masking-squad2'\n",
        "    model = BertForQuestionAnswering.from_pretrained(modelname).to(device)\n",
        "    tokenizer = BertTokenizer.from_pretrained(modelname)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d57a4368",
      "metadata": {
        "id": "d57a4368"
      },
      "source": [
        "## BERT Function <a id='BERT_training'></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ae9443da",
      "metadata": {
        "id": "ae9443da"
      },
      "outputs": [],
      "source": [
        "#create function that runs the BERT model\n",
        "def Run_BERT(question, text_batch):\n",
        "    \n",
        "    #encode the question and the paragraph(text)\n",
        "    input_ids = tokenizer.encode(question,text_batch,max_length=512)\n",
        "    \n",
        "    #search the input_ids for the first instance of the SEP token\n",
        "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
        "            \n",
        "    #Segment A occurs from the first char to the end of the SEP token instance\n",
        "    num_seg_a = sep_index+1\n",
        "    \n",
        "    #The rest of the tokens will belong to segment B\n",
        "    num_seg_b = len(input_ids)-num_seg_a\n",
        "    \n",
        "    #construct a list of 0's and 1's\n",
        "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "    \n",
        "    #there should be a segment id for every input token\n",
        "    #if this doesnt return an error we are good\n",
        "    assert len(segment_ids) == len(input_ids)\n",
        "    \n",
        "    #run the model using the current data\n",
        "    outputs = model(torch.as_tensor([input_ids]).to(device), #the tokens representing the input text \n",
        "                   token_type_ids=torch.as_tensor([segment_ids]).to(device), #the segment ids to differentiate Q from A\n",
        "                   return_dict=True)\n",
        "    \n",
        "    #get the start and end vectors\n",
        "    start_scores = outputs.start_logits\n",
        "    end_scores = outputs.end_logits\n",
        "    \n",
        "    #reconstruct the answer from the scores\n",
        "    answer_start = torch.argmax(start_scores).to(device)\n",
        "    answer_end = torch.argmax(end_scores).to(device)\n",
        "    \n",
        "    #get the string versions of the input tokens\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    \n",
        "    #create an answer variable and append the start of the first word\n",
        "    answer = tokens[answer_start]\n",
        "    \n",
        "    #fill out the remainder of the answer\n",
        "    for i in range(answer_start + 1, answer_end + 1):\n",
        "        #if we have a subword token, recombine it with the previous token\n",
        "        if(tokens[i][0:2]=='##'):\n",
        "            answer += tokens[i][2:]\n",
        "        elif(tokens[i][0]==','):\n",
        "            answer += tokens[i][0]\n",
        "        elif(tokens[i][0]=='\\''):\n",
        "            answer += tokens[i][0]\n",
        "        elif(tokens[i][0]=='-'):\n",
        "            answer += tokens[i][0]\n",
        "        elif(tokens[i][0]=='s'):\n",
        "            answer += tokens[i]\n",
        "        elif(tokens[i][0] == '.'):\n",
        "            answer += tokens[i][0]\n",
        "        elif(tokens[i][0].isnumeric() and i > 1):\n",
        "            if tokens[i-1][0]=='.':\n",
        "                answer += tokens[i][0]\n",
        "        else:\n",
        "            answer += ' ' + tokens[i]\n",
        "\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9b18942",
      "metadata": {
        "id": "e9b18942"
      },
      "source": [
        "## Helper Functions: Must run this block <a id='looping_helpers'></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "89ef0b66",
      "metadata": {
        "id": "89ef0b66"
      },
      "outputs": [],
      "source": [
        "def find_answer_context(paragraph, answer, buffer_sentences=1):\n",
        "    pgraph_chars = ''.join(paragraph.split(' '))\n",
        "    answer_chars = ''.join(answer.split(' '))\n",
        "    \n",
        "    # find the index in the character string where answer starts\n",
        "    answer_loc = pgraph_chars.find(answer_chars)\n",
        "    \n",
        "    # find all the indices of the periods\n",
        "    period_indices = [x for x in findall('.', pgraph_chars)]\n",
        "    \n",
        "    # find the index value where the answer would be inserted\n",
        "    stop_idx = np.searchsorted(period_indices, answer_loc)\n",
        "    \n",
        "    # find the periods marking to the left and right of the start point\n",
        "    if stop_idx > 0:\n",
        "        context_left = period_indices[stop_idx-1::-1]\n",
        "    else:\n",
        "        context_left = []\n",
        "    context_right = period_indices[stop_idx:-1]\n",
        "    \n",
        "    # loop through the periods until we find the appropraite number of buffer sentences worth\n",
        "    p_idx = 0\n",
        "    left_count = 0\n",
        "    while left_count <= buffer_sentences and p_idx < len(context_left):\n",
        "        if not pgraph_chars[context_left[p_idx]+1].isnumeric():\n",
        "            left_count +=1\n",
        "        p_idx += 1\n",
        "    \n",
        "    left_period_num = len(context_left) - p_idx\n",
        "    \n",
        "    p_idx = 0\n",
        "    right_count = 0\n",
        "    while right_count <= buffer_sentences and p_idx < len(context_right):\n",
        "        if not pgraph_chars[context_right[p_idx]+1].isnumeric():\n",
        "            right_count +=1\n",
        "        p_idx += 1\n",
        "    \n",
        "    right_period_num = len(context_left) + p_idx + 1\n",
        "  \n",
        "    # find the indices in the paragraph\n",
        "    left_p_idx = find_nth(paragraph, '.', left_period_num)+1\n",
        "    right_p_idx = find_nth(paragraph, '.', right_period_num)+1\n",
        "\n",
        "    return paragraph[left_p_idx:right_p_idx]\n",
        "    \n",
        "    \n",
        "def findall(p, s):\n",
        "    '''Yields all the positions of\n",
        "    the pattern p in the string s.'''\n",
        "    i = s.find(p)\n",
        "    while i != -1:\n",
        "        yield i\n",
        "        i = s.find(p, i+1)\n",
        "        \n",
        "def find_nth(haystack, needle, n):\n",
        "    start = haystack.find(needle)\n",
        "    while start >= 0 and n > 1:\n",
        "        start = haystack.find(needle, start+len(needle))\n",
        "        n -= 1\n",
        "    return start\n",
        "\n",
        "def sentences_with_answers(paragraphs, answers):\n",
        "    # storage data structures\n",
        "    answer_sentences = []\n",
        "    total_count = 0\n",
        "    for idx, ans in enumerate(answers):\n",
        "        text = find_answer_context(paragraphs[idx], ans)\n",
        "        if text != '':\n",
        "            answer_sentences.append(text)\n",
        "            total_count += len(find_answer_context(paragraphs[idx], ans).split(' '))\n",
        "    return total_count, answer_sentences\n",
        "\n",
        "def compress_corpus(documents, max_doc_size = 450):\n",
        "    size = 0\n",
        "    compressed_corp = []\n",
        "    current_doc = ''\n",
        "    for d in documents:\n",
        "        d_list = d\n",
        "        if size + len(d_list.split(' ')) < max_doc_size:\n",
        "            current_doc += ' ' + d\n",
        "            size += len(d_list.split(' '))\n",
        "        else:\n",
        "            compressed_corp.append(current_doc)\n",
        "            current_doc = d\n",
        "            size = len(d_list.split(' '))\n",
        "    compressed_corp.append(current_doc)\n",
        "    return compressed_corp\n",
        "\n",
        "def narrow_down_answers(question, documents, answers):\n",
        "    \n",
        "    while len(answers) > 1:\n",
        "        # back out sentences from the answers\n",
        "        counts, sentences = sentences_with_answers(documents, answers)\n",
        "        # compress the sentences down to a smaller number of documents\n",
        "        compressed_corpus = compress_corpus(sentences)\n",
        "\n",
        "        answers = []\n",
        "        documents = compressed_corpus.copy()\n",
        "        for paragraph in compressed_corpus:\n",
        "            # run BERT\n",
        "            BERT_answer = Run_BERT(question, paragraph)\n",
        "            \n",
        "            # check that BERT answer is acceptable before adding to answer list\n",
        "            if '[CLS]' not in BERT_answer:\n",
        "                answers.append(BERT_answer)\n",
        "\n",
        "    if len(answers) == 0:\n",
        "        return None\n",
        "    else:\n",
        "        return answers[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERT Standard Benchmarking <a id='standard_benchmarking'></a>\n"
      ],
      "metadata": {
        "id": "6jz8iRVFYeYH"
      },
      "id": "6jz8iRVFYeYH"
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples = 500\n",
        "\n",
        "random_indices = list(random.sample(range(0, len(questions_BERT)), num_samples))\n",
        "\n",
        "sample_questions_BERT = []\n",
        "for idx in random_indices:\n",
        "    sample_questions_BERT.append(questions_BERT[idx])\n",
        "\n",
        "# sample_questions_BERT = questions_BERT[0:num_samples]\n",
        "\n",
        "test_rounds = num_samples\n",
        "correct_answers = 0\n",
        "\n",
        "#define the stopwords\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "noncontext_words = sp.Defaults.stop_words\n",
        "# noncontext_words = ['the','i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
        "\n",
        "#loop over the sample questions\n",
        "count = 1\n",
        "index = 0\n",
        "for question in sample_questions_BERT:\n",
        "    BERT_answers = []\n",
        "    print(\"########################################################################################################\")\n",
        "    print(\"Correct Answers: \" + str(correct_answers))\n",
        "    print(\"Question #\" + str(count))\n",
        "    print(question)\n",
        "    \n",
        "    count += 1\n",
        "        \n",
        "    #get the correct answer to this question\n",
        "    correct_answer = question_answers_BERT[questions_BERT.index(question)]\n",
        "    print(\"Correct Answer: %s\" % (correct_answer))\n",
        "    if(type(correct_answer!=str)):\n",
        "        correct_answer = str(correct_answer)\n",
        "        \n",
        "    #get the top k paragraphs from the non punctuated list\n",
        "    #use the indices to retrieve the punctuated article that BERT wants\n",
        "    candidate_articles_BERT, doc_ret_idx = Retrieve_Article(question,articles_BERT,k=5)\n",
        "    \n",
        "    #segment the chosen candidate article in \"paragraphs\"\n",
        "    candidate_seg_articles = segment_documents(candidate_articles_BERT, max_doc_length=500)\n",
        "    \n",
        "    #retrieve which paragraph contains the correct answer\n",
        "#     top_paragraphs, doc_ret_idx = Retrieve_Article(question,candidate_seg_articles,useParagraphs=True,k=10)\n",
        "                \n",
        "    #return the answers from each of the top k paragraphs in descending order by relevancy\n",
        "    for segment in candidate_seg_articles:\n",
        "        BERT_prediction = Run_BERT(question, segment)   \n",
        "        if(BERT_prediction==\"[CLS]\"):\n",
        "            continue\n",
        "        if(\"[CLS]\" in BERT_prediction):\n",
        "            continue\n",
        "#         print(BERT_prediction)\n",
        "                        \n",
        "        BERT_answers.append(BERT_prediction)\n",
        "        \n",
        "        #check to see if the return type is a string\n",
        "        if((type(BERT_prediction)==str)):\n",
        "            #create lists of words for the predicted and the correct answers\n",
        "            BERT_pred_list = re.split('\\s+', BERT_prediction)\n",
        "            BERT_true_list = re.split('\\s+', correct_answer)\n",
        "            \n",
        "            BERT_pred_list_fix = []\n",
        "            BERT_true_list_fix = []\n",
        "            #remove the stop words in the lists\n",
        "            for word in BERT_pred_list:\n",
        "                if(word not in noncontext_words):\n",
        "                    BERT_pred_list_fix.append(word)\n",
        "                    \n",
        "            #remove the stop words in the lists\n",
        "            for word in BERT_true_list:\n",
        "                if(word not in noncontext_words):\n",
        "                    BERT_true_list_fix.append(word)\n",
        "                                            \n",
        "            #check to see if any words in the prediction are in the answer\n",
        "            true_ans_len = len(BERT_true_list_fix)\n",
        "            num_matches = 0\n",
        "            for word in BERT_pred_list_fix:\n",
        "                if(word in BERT_true_list_fix):\n",
        "                    num_matches += 1\n",
        "            \n",
        "            if(true_ans_len==1):\n",
        "                if(num_matches==true_ans_len):\n",
        "                    correct_answers += 1\n",
        "                    print(\"BERT Predicted Answer: \" + BERT_prediction)\n",
        "                    break\n",
        "            else:\n",
        "                if(num_matches>=round(0.5*true_ans_len)):\n",
        "                    correct_answers += 1\n",
        "                    print(\"BERT Predicted Answer: \" + BERT_prediction)\n",
        "                    break\n",
        "\n",
        "                    \n",
        "BERT_accuracy = round(correct_answers/test_rounds,2)\n",
        "print(BERT_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BmyJaAWYdH8",
        "outputId": "db6953b6-0bec-409c-f9e3-5de8ceccd37d"
      },
      "id": "_BmyJaAWYdH8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "########################################################################################################\n",
            "Correct Answers: 0\n",
            "Question #1\n",
            "What group famously enjoyed themselves on Union Street?\n",
            "Correct Answer: sailors from the Royal Navy\n",
            "########################################################################################################\n",
            "Correct Answers: 0\n",
            "Question #2\n",
            "In what century did the Ottoman's start to desire foreign manuscripts?\n",
            "Correct Answer: 15th Century\n",
            "BERT Predicted Answer: 15th Century\n",
            "########################################################################################################\n",
            "Correct Answers: 1\n",
            "Question #3\n",
            "What had to be evacuated due to potential flooding?\n",
            "Correct Answer: Entire villages\n",
            "BERT Predicted Answer: villages\n",
            "########################################################################################################\n",
            "Correct Answers: 2\n",
            "Question #4\n",
            "A mammal's brain is how many times larger than a birds relative to body size?\n",
            "Correct Answer: twice as large\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7328d9db",
      "metadata": {
        "id": "7328d9db"
      },
      "source": [
        "## BERT Benchmarking with Looping <a id='benchmarking_BERT_with_loop'></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "00e2a2c8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00e2a2c8",
        "outputId": "a2a23805-64e4-497b-fb5d-66e4f57b4078"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "####################################################################################\n",
            "What reaction describes this process?\n",
            "Correct Answer: Schikorr reaction\n",
            "BERT Answer: process theologians view God as \" the fellowsufferer who understands \", and as the being who issupremely affected by temporal events. Hartshorne points out that people would not praise a human ruler who was unaffected by either the joys orsorrows of his followers-so why would this be a praise- worthy quality in God ? Instead, as the being who is most affected by the world, God is the being who can most appropriately respond to the world. Montini and Angelo Roncalli were considered to be friends, but when Roncalli, as Pope John XXIII announced a new Ecumenical Council, Cardinal Montini reacted with disbelief\n",
            "####################################################################################\n",
            "In what year did Planck receive the Nobel Prize in Physics for his discovery of energy quanta?\n",
            "Correct Answer: 1918\n",
            "BERT Answer: 1921\n",
            "####################################################################################\n",
            "What technique do they use to make animal habitats?\n",
            "Correct Answer: slash-and-burn\n",
            "BERT Answer: None\n",
            "####################################################################################\n",
            "Where did the name Switzer originate from?\n",
            "Correct Answer: the Alemannic Schwiizer\n",
            "BERT Answer: Alemannic Schwiizer\n",
            "####################################################################################\n",
            "What was the traditional social organization of the Marshall Islanders?\n",
            "Correct Answer: Matrilineality\n",
            "BERT Answer: Matrilineality\n",
            "####################################################################################\n",
            "What country originally pulled iPods due to higher-than-allowed volume levels?\n",
            "Correct Answer: France\n",
            "BERT Answer: Belgium\n",
            "####################################################################################\n",
            "What is the Faraday constant?\n",
            "Correct Answer: the charge of one mole of electrons\n",
            "BERT Answer: the charge of one mole of electrons\n",
            "####################################################################################\n",
            "From what land did the people of Niutao believe they came?\n",
            "Correct Answer: Samoa\n",
            "BERT Answer: migration into the Pacific\n",
            "####################################################################################\n",
            "Who was elected to be Chairman in May?\n",
            "Correct Answer: Mutalibov\n",
            "BERT Answer: None\n",
            "####################################################################################\n",
            "In what year did Sven-Goran Eriksson become the manager of England's football team?\n",
            "Correct Answer: 2001\n",
            "BERT Answer: None\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import re\n",
        "import spacy\n",
        "import nltk\n",
        "\n",
        "for _ in range(100):\n",
        "    record_questions = []\n",
        "    record_correct_answer = []\n",
        "    record_answers = []\n",
        "    record_segments = []\n",
        "    record_articles = []\n",
        "\n",
        "    random_indices = list(random.sample(range(0, len(questions_BERT)), 1))\n",
        "\n",
        "    sample_questions = []\n",
        "    for idx in random_indices:\n",
        "        sample_questions.append(questions_BERT[idx])\n",
        "\n",
        "    # sample_questions = questions_BERT[0:2]\n",
        "\n",
        "    test_rounds = len(sample_questions)\n",
        "    correct_answers = 0\n",
        "\n",
        "    #define the stopwords\n",
        "    sp = spacy.load('en_core_web_sm')\n",
        "    noncontext_words = sp.Defaults.stop_words\n",
        "    # noncontext_words = ['the','i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
        "\n",
        "    #loop over the sample questions\n",
        "    for question in sample_questions:\n",
        "        BERT_answers = []\n",
        "        print(\"####################################################################################\")\n",
        "        print(question)\n",
        "\n",
        "        # Dan added dictionary to look up segment\n",
        "        lookup_segment = {}\n",
        "        segments = []\n",
        "        answers_to_consider = []\n",
        "        current = 0\n",
        "\n",
        "        #get the correct answer to this question\n",
        "        correct_answer = question_answers_BERT[questions_BERT.index(question)]\n",
        "        print(\"Correct Answer: %s\" % (correct_answer))\n",
        "        record_correct_answer.append(correct_answer)\n",
        "\n",
        "        #get the top k paragraphs\n",
        "        candidate_articles, article_indices = Retrieve_Article(question,articles_BERT,10)\n",
        "        record_articles.append(candidate_articles)\n",
        "\n",
        "        #segment the chosen candidate article in \"paragraphs\"\n",
        "        candidate_seg_articles = segment_documents(candidate_articles, max_doc_length=450)\n",
        "\n",
        "        #return the answers from each of the top k paragraphs in descending order by relevancy\n",
        "        for seg_idx, segment in enumerate(candidate_seg_articles):\n",
        "            #BERT_prediction, start_idx, end_idx = Run_BERT(question, segment)\n",
        "            BERT_prediction = Run_BERT(question, segment)\n",
        "\n",
        "            BERT_answers.append(BERT_prediction)\n",
        "\n",
        "            ## Dan added this for troubleshooting next part\n",
        "            if '[CLS]' not in BERT_prediction:\n",
        "                # store segment information\n",
        "                lookup_segment[seg_idx] = current\n",
        "                segments.append(segment)\n",
        "\n",
        "                # store answer information extracted from text\n",
        "                answers_to_consider.append(BERT_prediction)\n",
        "\n",
        "\n",
        "                # increment index for dictionary\n",
        "                current +=1\n",
        "            #check to see if the return type is a string\n",
        "            if(type(BERT_prediction)==str):\n",
        "                #create lists of words for the predicted and the correct answers\n",
        "                BERT_pred_list = re.split('\\s+', BERT_prediction)\n",
        "                BERT_true_list = re.split('\\s+', correct_answer)\n",
        "\n",
        "                BERT_pred_list_fix = []\n",
        "                BERT_true_list_fix = []\n",
        "                #remove the stop words in the lists\n",
        "                for word in BERT_pred_list:\n",
        "                    if(word not in noncontext_words):\n",
        "                        BERT_pred_list_fix.append(word)\n",
        "\n",
        "                #remove the stop words in the lists\n",
        "                for word in BERT_true_list:\n",
        "                    if(word not in noncontext_words):\n",
        "                        BERT_true_list_fix.append(word)\n",
        "\n",
        "                #check to see if any words in the prediction are in the answer\n",
        "                true_ans_len = len(BERT_true_list_fix)\n",
        "                num_matches = 0\n",
        "                for word in BERT_pred_list_fix:\n",
        "                    if(word in BERT_true_list_fix):\n",
        "                        num_matches += 1\n",
        "\n",
        "                if(true_ans_len==1):\n",
        "                    if(num_matches==true_ans_len):\n",
        "                        correct_answers += 1\n",
        "                else:\n",
        "                    if(num_matches>=round(0.5*true_ans_len)):\n",
        "                        correct_answers += 1\n",
        "        \n",
        "        # BERT_accuracy = correct_answers/test_rounds\n",
        "        # print(BERT_accuracy)\n",
        "        \n",
        "        record_questions.append(question)\n",
        "        record_answers.append(answers_to_consider)\n",
        "        record_segments.append(segments)\n",
        "\n",
        "        final_answer = narrow_down_answers(question, segments, answers_to_consider)   \n",
        "        print('BERT Answer:', final_answer)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "CS247_CourseProject_Benchmarking_Code.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}