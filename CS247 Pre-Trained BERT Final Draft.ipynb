{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9697c95a",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "Code below here is segmented by functionality and what was performed in each part. For running our scripts in this notebook, you will need to modify the file path in the data import section [here](#import_training_data) to wherever the excel sheets containing the training sets have been set. It is important that the two excel sheets are saved in the same file location otherwise a second path will need to be specified in the cell at the link above.\n",
    "\n",
    "### Python Imports\n",
    "   * [Link to package import](#import_packages)\n",
    "   * [Link to training data import](#import_training_data)\n",
    "   \n",
    "### Data Formatting\n",
    "* [Article List Formating](#generate_python_list)\n",
    "* [Article Retrieval Code](#retrieve_articles)\n",
    "\n",
    "### Article Retrieval (TF-IDF)\n",
    "* [Data Preprocessing](#preprocess_data)\n",
    "* [Document Retrieval Benchmarking](#document_retrieval_benchmarking)\n",
    "* [Pretrained Model](#import_pretrained_model)\n",
    "\n",
    "### Answer Retrieval (BERT)\n",
    "* [BERT Implementation](#BERT_training)\n",
    "* [BERT Looping Helper Functions](#looping_helpers)\n",
    "* [Benchmarking BERT with Looping](#benchmarking_BERT_with_loop)\n",
    "* [BERT Benchmarking Troubleshooting](#BERT_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e358270",
   "metadata": {},
   "source": [
    "## Import Python Packages <a id='import_packages'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "38ea1f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "# specific functionality\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "# import BERT models from transformers\n",
    "import transformers\n",
    "from transformers import BertTokenizer, AutoTokenizer, BertForQuestionAnswering, BertTokenizerFast, BertConfig, DistilBertForQuestionAnswering, DistilBertTokenizerFast\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9908963b",
   "metadata": {},
   "source": [
    "## Import Training Sets <a id='import_training_data'></a>\n",
    "\n",
    "### ** IMPORTANT FOR RUNNING ** Need to change the file path to match the directory where the excel sheets containing the training data have been saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "d4577a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_dataframe = False\n",
    "\n",
    "# must change this path to match file location\n",
    "savepath = r'C:\\Users\\drhulbert\\Documents\\Repos\\CS_247-Project-Code-main\\CS_247-Project-Code-main\\\\'\n",
    "\n",
    "# Import the excel sheet containing the BERT training data\n",
    "training_BERT_filename = \"squad_training_dataset_BERT.csv\"\n",
    "save_BERT_fullpath = os.path.join(savepath,training_BERT_filename)\n",
    "training_BERT_dataset_df = pd.read_csv(save_BERT_fullpath)\n",
    "\n",
    "training_docretr_filename = \"squad_training_dataset_docretr.csv\"\n",
    "save_docretr_fullpath = os.path.join(savepath,training_docretr_filename)\n",
    "training_docretr_dataset_df = pd.read_csv(save_docretr_fullpath)\n",
    "\n",
    "# drop non-answers from the dataset\n",
    "training_BERT_dataset_df = training_BERT_dataset_df.dropna()\n",
    "\n",
    "# display the training set dataframe\n",
    "if display_dataframe:\n",
    "    display(training_BERT_dataset_df)\n",
    "    display(training_docretr_dataset_df)\n",
    "    display(training_BERT_dataset_df)\n",
    "\n",
    "# create training lists\n",
    "questions_BERT = training_BERT_dataset_df['question'].to_list()\n",
    "answers_BERT = training_BERT_dataset_df['answer'].to_list()\n",
    "topics_BERT = training_BERT_dataset_df['topic'].to_list()\n",
    "answers_start = training_BERT_dataset_df['answer_start'].to_list()\n",
    "\n",
    "# Import training data for article to questions\n",
    "topic_to_article_BERT_filename = \"topics_to_articles_BERT.csv\"\n",
    "save_fullpath = os.path.join(savepath,topic_to_article_BERT_filename)\n",
    "topics_articles_BERT_df = pd.read_csv(save_fullpath)\n",
    "\n",
    "#get the unique topic names from the dataframe\n",
    "topic_strings_BERT = topics_articles_BERT_df.columns.to_list()\n",
    "#get the correct label for the questions\n",
    "question_topic_BERT = training_BERT_dataset_df['topic'].to_list()\n",
    "#get the correct answers for questions\n",
    "question_answers_BERT = training_BERT_dataset_df['answer'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a99c773",
   "metadata": {},
   "source": [
    "## Generate List of Articles <a id='generate_python_list'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "38dc9999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Article(df,topic):\n",
    "    article = df[topic].to_list()[0]\n",
    "    \n",
    "    return article\n",
    "\n",
    "articles_BERT = []\n",
    "for topic in topic_strings_BERT:\n",
    "    articles_BERT.append(Get_Article(topics_articles_BERT_df,topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "822a698a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_documents(docs, max_doc_length=500):\n",
    "    # List containing full and segmented docs\n",
    "    segmented_docs = []\n",
    "\n",
    "    for doc in docs:\n",
    "        # Split document by spaces to obtain a word count that roughly approximates the token count\n",
    "        split_to_words = doc.split(\" \")\n",
    "\n",
    "        # If the document is longer than our maximum length, split it up into smaller segments and add them to the list \n",
    "        if len(split_to_words) > max_doc_length:\n",
    "            for doc_segment in range(0, len(split_to_words), max_doc_length):\n",
    "                segmented_docs.append(\" \".join(split_to_words[doc_segment:doc_segment + max_doc_length]))\n",
    "\n",
    "        # If the document is shorter than our maximum length, add it to the list\n",
    "        else:\n",
    "            segmented_docs.append(doc)\n",
    "\n",
    "    return segmented_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "ac545b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beyonce Giselle Knowles-Carter (/bi:'janseI/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyonce's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".Following the disbandment of Destiny's Child in June 2005, she released her second solo album, B'Day (2006), which contained hits \"Deja Vu\", \"Irreplaceable\", and \"Beautiful Liar\". Beyonce also ventured into acting, with a Golden Globe-nominated performance in Dreamgirls (2006), and starring roles in The Pink Panther (2006) and Obsessed (2009). Her marriage to rapper Jay Z and portrayal of Etta James in Cadillac Records (2008) influenced her third album, I Am... Sasha Fierce (2008), which saw the birth of her alter-ego Sasha Fierce and earned a record-setting six Grammy Awards in 2010, including Song of the Year for \"Single Ladies (Put a Ring on It)\". Beyonce took a hiatus from music in 2010 and took over management of her career; her fourth album 4 (2011) was subsequently mellower in tone, exploring 1970s funk, 1980s pop, and 1990s soul. Her critically acclaimed fifth studio album, Beyonce (2013), was distinguished from previous releases by its experimental production and exploration of darker themes.A self-described \"modern-day feminist\", Beyonce creates songs that are often characterized by themes of love, relationships, and monogamy, as well as female sexuality and empowerment. On stage, her dynamic, highly choreographed performances have led to critics hailing her as one of the best entertainers in contemporary popular music. Throughout a career spanning 19 years, she has sold over 118 million records as a solo artist, and a further 60 million with Destiny's Child, making her one of the best-selling music artists of all time. She has won 20 Grammy Awards and is the most nominated woman in the award's history. The Recording Industry Association of America recognized her as the Top Certified Artist in America during the 2000s decade. In 2009, Billboard named her the Top Radio Songs Artist of the Decade, the Top Female Artist of the 2000s and their Artist of the Millennium in 2011. Time listed her among the 100 most influential people in the world in 2013 and 2014. Forbes magazine also listed her as the most powerful female musician of 2015.Beyonce Giselle Knowles was born in Houston, Texas, to Celestine Ann \"Tina\" Knowles (nee Beyince), a hairdresser and salon owner, and Mathew Knowles, a Xerox sales manager. Beyonce's name is a tribute to her mother's maiden name. Beyonce's younger sister Solange is also a singer and a former member of Destiny's Child. Mathew is African-American, while Tina is\n"
     ]
    }
   ],
   "source": [
    "segmented_articles_BERT = segment_documents(articles_BERT)\n",
    "print(segmented_articles_BERT[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee07c85",
   "metadata": {},
   "source": [
    "## Article Retrieval Code <a id='retrieve_articles'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "7339dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make pre-processing functions\n",
    "def convert_lower_case(data):\n",
    "    return str(np.char.lower(data))\n",
    "\n",
    "def remove_punctuation(data):\n",
    "    new_data = \"\"\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in symbols:\n",
    "        new_data = np.char.replace(data, i, ' ')\n",
    "        \n",
    "    return(str(new_data))\n",
    "def remove_apostrophe(data):\n",
    "    return str(np.char.replace(data, \"'\", \"\"))\n",
    "\n",
    "def remove_single_characters(data):\n",
    "    new_text = \"\"\n",
    "    \n",
    "    word_list = nltk.word_tokenize(data)\n",
    "    \n",
    "    for w in word_list:\n",
    "        if len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    \n",
    "    return new_text\n",
    "\n",
    "def Lemmatize(data):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    word_list = nltk.word_tokenize(data)\n",
    "    \n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    \n",
    "    return lemmatized_output\n",
    "def Stemming(data):\n",
    "    ps = PorterStemmer()\n",
    "    \n",
    "    word_list = nltk.word_tokenize(data)\n",
    "    \n",
    "    stem_output = ' '.join([ps.stem(w) for w in word_list])\n",
    "    \n",
    "    return stem_output\n",
    "\n",
    "#create a function to preprocess the data\n",
    "def Pre_Process_Data(data):\n",
    "    new_data = remove_punctuation(data)\n",
    "    return new_data\n",
    "\n",
    "def Retrieve_Article(query, docs, k=5):\n",
    "    #pre-process the query\n",
    "    query = Pre_Process_Data(query)\n",
    "    \n",
    "    query_words = re.split('\\s+', query)\n",
    "    num_cols = len(query_words)\n",
    "    \n",
    "    # Initialize a vectorizer that removes English stop words\n",
    "    vectorizer = TfidfVectorizer(analyzer=\"word\", stop_words='english',sublinear_tf=True,use_idf=True)\n",
    "    \n",
    "    # Create a corpus of query and documents and convert to TFIDF vectors\n",
    "    query_and_docs = [query] + docs\n",
    "    matrix = vectorizer.fit_transform(query_and_docs)\n",
    "    \n",
    "    #apply SVD to the TF-IDF vectorized matrix\n",
    "    svd = TruncatedSVD(n_components=num_cols+200,n_iter=1,random_state=42)\n",
    "    \n",
    "    #fit and transform the SVD model\n",
    "    matrix_new = svd.fit_transform(matrix)\n",
    "    matrix_new = csr_matrix(matrix_new)\n",
    "\n",
    "    # Holds our cosine similarity scores\n",
    "    scores = []\n",
    "\n",
    "    # The first vector is our query text, so compute the similarity of our query against all document vectors\n",
    "    for i in range(1, len(query_and_docs)):\n",
    "        scores.append(cosine_similarity(matrix_new[0], matrix_new[i])[0][0])\n",
    "\n",
    "    # Sort list of scores and return the top k highest scoring documents\n",
    "    sorted_list = sorted(enumerate(scores), key=lambda x: x[1], reverse=True)\n",
    "    top_doc_indices = [x[0] for x in sorted_list[:k]]\n",
    "    top_docs = [docs[x] for x in top_doc_indices]\n",
    "\n",
    "    return top_docs, top_doc_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d9307d",
   "metadata": {},
   "source": [
    "## Preprocess the Data before Retrieval <a id='preprocess_data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "22536995",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_articles_BERT = []\n",
    "for doc in articles_BERT:\n",
    "    preprocess_articles_BERT.append(Pre_Process_Data(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e393651a",
   "metadata": {},
   "source": [
    "## Benchmark Document Retrieval <a id='document_retrieval_benchmarking'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "67d69dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving the top 5 articles for each question...\n",
      "Question #1/10\n",
      "Question #2/10\n",
      "Question #3/10\n",
      "Question #4/10\n",
      "Question #5/10\n",
      "Question #6/10\n",
      "Question #7/10\n",
      "Question #8/10\n",
      "Question #9/10\n",
      "Question #10/10\n",
      "k=5 Article Retrieval Accuracy: 0.8\n"
     ]
    }
   ],
   "source": [
    "#conduct document retrieval for each question in the dataset\n",
    "\n",
    "def Benchmark_DocRetrieval(questions,question_topic,articles,articles_true,topic_strings,RandQs=False):\n",
    "    \n",
    "    if(RandQs):\n",
    "        random_indices = list(random.sample(range(0, len(questions)), 10))\n",
    "\n",
    "        sample_questions = []\n",
    "        for idx in random_indices:\n",
    "            sample_questions.append(questions[idx])\n",
    "    else:\n",
    "        #create a sample of questions\n",
    "        sample_questions = questions[0:10]\n",
    "    \n",
    "    total = len(sample_questions)\n",
    "    correct = 0\n",
    "    tracker = 1\n",
    "    for question in sample_questions:\n",
    "        #create a tracker\n",
    "        print(\"Question #%d/%d\" %(tracker,total))\n",
    "\n",
    "        #get the true label for the question\n",
    "        true_question_label = question_topic[questions.index(question)]\n",
    "\n",
    "        #run the doc retriever on the current question \n",
    "        top_articles, predicted_articles_indices = Retrieve_Article(question,articles,k=5)\n",
    "       \n",
    "        #iterate over all predicted articles and check if the prediction is correct\n",
    "        for prediction in top_articles:    \n",
    "            #get the true topic of the predicted article\n",
    "            true_article_label = topic_strings[articles_true.index(prediction)]\n",
    "\n",
    "            #this will handle if the correct article is even chosen\n",
    "            if(true_question_label==true_article_label):\n",
    "                correct += 1\n",
    "\n",
    "        tracker += 1\n",
    "\n",
    "    return correct/total\n",
    "\n",
    "print(\"Retrieving the top 5 articles for each question...\")\n",
    "retrieval_accuracy = Benchmark_DocRetrieval(questions_BERT,question_topic_BERT,articles_BERT,articles_BERT,topic_strings_BERT,RandQs=True)\n",
    "print(\"k=5 Article Retrieval Accuracy: \" + str(retrieval_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6605d7f3",
   "metadata": {},
   "source": [
    "## Import the PreTrained Model <a id='import_pretrained_model'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "296502f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "# modelname = 'bert-large-uncased-whole-word-masking-finetuned-squad'\n",
    "modelname = 'deepset/bert-base-cased-squad2'\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained(modelname)\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5591fc8f",
   "metadata": {},
   "source": [
    "## BERT Function <a id='BERT_training'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "d79370af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function that runs the BERT model\n",
    "def Run_BERT(question, text_batch):\n",
    "    \n",
    "    #encode the question and the paragraph(text)\n",
    "    input_ids = tokenizer.encode(question,text_batch,max_length=512)\n",
    "    \n",
    "    #search the input_ids for the first instance of the SEP token\n",
    "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "            \n",
    "    #Segment A occurs from the first char to the end of the SEP token instance\n",
    "    num_seg_a = sep_index+1\n",
    "    \n",
    "    #The rest of the tokens will belong to segment B\n",
    "    num_seg_b = len(input_ids)-num_seg_a\n",
    "    \n",
    "    #construct a list of 0's and 1's\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "    \n",
    "    #there should be a segment id for every input token\n",
    "    #if this doesnt return an error we are good\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "    \n",
    "    #run the model using the current data\n",
    "    outputs = model(torch.tensor([input_ids]), #the tokens representing the input text \n",
    "                   token_type_ids=torch.tensor([segment_ids]), #the segment ids to differentiate Q from A\n",
    "                   return_dict=True)\n",
    "    \n",
    "    #get the start and end vectors\n",
    "    start_scores = outputs.start_logits\n",
    "    end_scores = outputs.end_logits\n",
    "    \n",
    "    #reconstruct the answer from the scores\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "    \n",
    "    #get the string versions of the input tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    \n",
    "    #create an answer variable and append the start of the first word\n",
    "    answer = tokens[answer_start]\n",
    "    \n",
    "    #fill out the remainder of the answer\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "        #if we have a subword token, recombine it with the previous token\n",
    "        if(tokens[i][0:2]=='##'):\n",
    "            answer += tokens[i][2:]\n",
    "        elif(tokens[i][0]==','):\n",
    "            answer += tokens[i][0]\n",
    "        elif(tokens[i][0]=='\\''):\n",
    "            answer += tokens[i][0]\n",
    "        elif(tokens[i][0]=='-'):\n",
    "            answer += tokens[i][0]\n",
    "        elif(tokens[i][0]=='s'):\n",
    "            answer += tokens[i]\n",
    "        elif(tokens[i][0] == '.'):\n",
    "            answer += tokens[i][0]\n",
    "        elif(tokens[i][0].isnumeric() and i > 1):\n",
    "            if tokens[i-1][0]=='.':\n",
    "                answer += tokens[i][0]\n",
    "        else:\n",
    "            answer += ' ' + tokens[i]\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784319d6",
   "metadata": {},
   "source": [
    "## Helper Functions <a id='looping_helpers'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "2c8c67cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given a paragraph and answer from BERT will return the exact answer context based on periods in the paragraph\n",
    "def find_answer_context(paragraph, answer, buffer_sentences=0):\n",
    "    pgraph_chars = ''.join(paragraph.split(' '))\n",
    "    answer_chars = ''.join(answer.split(' '))\n",
    "    \n",
    "    # find the index in the character string where answer starts\n",
    "    answer_loc = pgraph_chars.find(answer_chars)\n",
    "    \n",
    "    # find all the indices of the periods\n",
    "    period_indices = [x for x in findall('.', pgraph_chars)]\n",
    "    \n",
    "    # find the index value where the answer would be inserted\n",
    "    stop_idx = np.searchsorted(period_indices, answer_loc)\n",
    "    \n",
    "    # find the periods marking to the left and right of the start point\n",
    "    if stop_idx > 0:\n",
    "        context_left = period_indices[stop_idx-1::-1]\n",
    "    else:\n",
    "        context_left = []\n",
    "    context_right = period_indices[stop_idx:-1]\n",
    "    \n",
    "    # loop through the periods until we find the appropraite number of buffer sentences worth\n",
    "    p_idx = 0\n",
    "    left_count = 0\n",
    "    while left_count <= buffer_sentences and p_idx < len(context_left):\n",
    "        if not pgraph_chars[context_left[p_idx]+1].isnumeric():\n",
    "            left_count +=1\n",
    "        p_idx += 1\n",
    "    \n",
    "    left_period_num = len(context_left) - p_idx\n",
    "    \n",
    "    p_idx = 0\n",
    "    right_count = 0\n",
    "    while right_count <= buffer_sentences and p_idx < len(context_right):\n",
    "        if not pgraph_chars[context_right[p_idx]+1].isnumeric():\n",
    "            right_count +=1\n",
    "        p_idx += 1\n",
    "    \n",
    "    right_period_num = len(context_left) + p_idx + 1\n",
    "  \n",
    "    # find the indices in the paragraph\n",
    "    left_p_idx = find_nth(paragraph, '.', left_period_num)+1\n",
    "    right_p_idx = find_nth(paragraph, '.', right_period_num)+1\n",
    "\n",
    "    return paragraph[left_p_idx:right_p_idx]\n",
    "    \n",
    "\n",
    "# returns all the period indices in the string s\n",
    "def findall(p, s):\n",
    "    '''Yields all the positions of\n",
    "    the pattern p in the string s.'''\n",
    "    i = s.find(p)\n",
    "    while i != -1:\n",
    "        yield i\n",
    "        i = s.find(p, i+1)\n",
    "\n",
    "# find the nth occurance of needle in the haystack\n",
    "def find_nth(haystack, needle, n):\n",
    "    start = haystack.find(needle)\n",
    "    while start >= 0 and n > 1:\n",
    "        start = haystack.find(needle, start+len(needle))\n",
    "        n -= 1\n",
    "    return start\n",
    "\n",
    "# returns the sentences from the paragraphs that contain the answers\n",
    "def sentences_with_answers(paragraphs, answers):\n",
    "    # storage data structures\n",
    "    answer_sentences = []\n",
    "    total_count = 0\n",
    "    for idx, ans in enumerate(answers):\n",
    "        text = find_answer_context(paragraphs[idx], ans)\n",
    "        if text != '':\n",
    "            answer_sentences.append(text)\n",
    "            total_count += len(find_answer_context(paragraphs[idx], ans).split(' '))\n",
    "    return total_count, answer_sentences\n",
    "\n",
    "# combines the documents into a compressed corpus\n",
    "def compress_corpus(documents, max_doc_size = 450):\n",
    "    size = 0\n",
    "    compressed_corp = []\n",
    "    current_doc = ''\n",
    "    for d in documents:\n",
    "        d_list = d\n",
    "        if size + len(d_list.split(' ')) < max_doc_size:\n",
    "            current_doc += ' ' + d\n",
    "            size += len(d_list.split(' '))\n",
    "        else:\n",
    "            compressed_corp.append(current_doc)\n",
    "            current_doc = d\n",
    "            size = len(d_list.split(' '))\n",
    "    compressed_corp.append(current_doc)\n",
    "    return compressed_corp\n",
    "\n",
    "# runs BERT repeatedly until a single answer is output (this answer can be none)\n",
    "def narrow_down_answers(question, documents, answers):\n",
    "    \n",
    "    while len(answers) > 1:\n",
    "        # back out sentences from the answers\n",
    "        counts, sentences = sentences_with_answers(documents, answers)\n",
    "        # compress the sentences down to a smaller number of documents\n",
    "        compressed_corpus = compress_corpus(sentences)\n",
    "\n",
    "        answers = []\n",
    "        documents = compressed_corpus.copy()\n",
    "        for paragraph in compressed_corpus:\n",
    "            # run BERT\n",
    "            BERT_answer = Run_BERT(question, paragraph)\n",
    "            \n",
    "            # check that BERT answer is acceptable before adding to answer list\n",
    "            if '[CLS]' not in BERT_answer:\n",
    "                answers.append(BERT_answer)\n",
    "\n",
    "    if len(answers) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return answers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7950f6c",
   "metadata": {},
   "source": [
    "## Benchmarking BERT with Looping <a id='benchmarking_BERT_with_loop'></a>\n",
    "Individual benchmark talleying was completed by hand based on the outputs to the juptery cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "8cdf2286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################\n",
      "Question: What can prevent a green color in glass?\n",
      "Question Index: 62814\n",
      "Correct Answer: Manganese dioxide\n",
      "BERT Answer: systems that do not invoke it\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: Where is a royal assent ceremony held within the United Kingdom?\n",
      "Question Index: 58508\n",
      "Correct Answer: Palace of Westminster\n",
      "BERT Answer: Palace of Westminster\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: Where did Bell and his wife go on their honeymoon?\n",
      "Question Index: 10643\n",
      "Correct Answer: Europe\n",
      "BERT Answer: Quebec\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: Why do muslim anti-masonics believe that the Freemasons want to destroy the Al-Aqsa Mosque?\n",
      "Question Index: 35350\n",
      "Correct Answer: rebuild the Temple of Solomon in Jerusalem\n",
      "BERT Answer: to rebuild the Temple of Solomon in Jerusalem\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: A colder city like Lugo has how many days with frosts?\n",
      "Question Index: 54370\n",
      "Correct Answer: 40\n",
      "BERT Answer: 40\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: Buddha recommended viewing thing by how many marks of existence?\n",
      "Question Index: 5039\n",
      "Correct Answer: three\n",
      "BERT Answer: three\n",
      "125.0\n",
      "####################################################################################\n",
      "Question: In which year did Imperial University claim the award for being voted the highest in the UK for Job Prospects?\n",
      "Question Index: 25307\n",
      "Correct Answer: 2014\n",
      "BERT Answer: None\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: Give an example of someone who was a caretaker manager?\n",
      "Question Index: 74428\n",
      "Correct Answer: examples include Paul Hart at Portsmouth and David Pleat at Tottenham Hotspur.\n",
      "BERT Answer: Paul Hart\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: In what century was the process of using hops to produce beer introduced to England?\n",
      "Question Index: 10823\n",
      "Correct Answer: 15th\n",
      "BERT Answer: None\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: What design did Peter Cooper feel was the most efficient?\n",
      "Question Index: 68672\n",
      "Correct Answer: cylindrical\n",
      "BERT Answer: None\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: Who is Leucothea's sister?\n",
      "Question Index: 62053\n",
      "Correct Answer: Clytia\n",
      "BERT Answer: Artemis\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: The Belgian Suite is located at the bottom of which staircase?\n",
      "Question Index: 37062\n",
      "Correct Answer: Minister's Staircase\n",
      "BERT Answer: Belgian\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: Who played the mandolin in Vaudeville? \n",
      "Question Index: 60828\n",
      "Correct Answer: Samuel Siegel\n",
      "BERT Answer: Samuel Siegel\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: Where did Beatrice meet and fall in love with Prince Henry? \n",
      "Question Index: 34887\n",
      "Correct Answer: at the wedding of Victoria's granddaughter Princess Victoria of Hesse\n",
      "BERT Answer: at\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: Tajwid describes rules for what noiseless aspect of articulating Quranic verses?\n",
      "Question Index: 70694\n",
      "Correct Answer: pause in recitation\n",
      "BERT Answer: verses\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: What film depicts the Chicago Cubs defeating a baseball team from Miami in the 2015 World Series?\n",
      "Question Index: 37846\n",
      "Correct Answer: Back to the Future Part II\n",
      "BERT Answer: Back to the Future Part II\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: The Book of Confessions reflects the inclusion of another confession, what is it called?\n",
      "Question Index: 41701\n",
      "Correct Answer: Westminster Standards\n",
      "BERT Answer: Westminster Confession of Faith\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: What Gran Turismo game was shown in 2007 but not released until after 2007?\n",
      "Question Index: 58334\n",
      "Correct Answer: Gran Turismo 5 Prologue\n",
      "BERT Answer: None\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: What style of building is the Montevideo Metropolitan Cathedral? \n",
      "Question Index: 36370\n",
      "Correct Answer: Neoclassical\n",
      "BERT Answer: Neoclassical\n",
      "2.0\n",
      "####################################################################################\n",
      "Question: Why was the rule of precedent allowed?\n",
      "Question Index: 43223\n",
      "Correct Answer: in the absence of case law, it would be completely unworkable for every minor issue in every legal case to be briefed, argued, and decided\n",
      "BERT Answer: it\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: Who backed out of becoming SoS after Hillary?\n",
      "Question Index: 65843\n",
      "Correct Answer: Susan Rice\n",
      "BERT Answer: None\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: Who would rule the country temporarily?\n",
      "Question Index: 58889\n",
      "Correct Answer: a transitional parliament\n",
      "BERT Answer: United States President\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: Why did many software developers choose not to redesign the Mac operating system and rewrite the programming code?\n",
      "Question Index: 28867\n",
      "Correct Answer: This was a time-consuming task\n",
      "BERT Answer: time\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: Is teenage alcohol drug use at al all-time high or low?\n",
      "Question Index: 30465\n",
      "Correct Answer: low\n",
      "BERT Answer: None\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: In what films did Spielberg address war?\n",
      "Question Index: 68374\n",
      "Correct Answer: Empire of the Sun, Saving Private Ryan, War Horse and Bridge of Spies\n",
      "BERT Answer: Empire of the Sun, Saving Private Ryan, War Horse and Bridge of Spies\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: What indirect benefit of DST might cause some areas to observe it even though they don't get any direct benefits like cost or energy savings?\n",
      "Question Index: 13997\n",
      "Correct Answer: coordination with others\n",
      "BERT Answer: heart attack\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: Which year brought the end to silver being in dimes and quarters?\n",
      "Question Index: 25165\n",
      "Correct Answer: 1964\n",
      "BERT Answer: 2015\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: Where did Arabic numerals originate?\n",
      "Question Index: 49202\n",
      "Correct Answer: India\n",
      "BERT Answer: None\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: What are two smaller galleries in Nanjing?\n",
      "Question Index: 14807\n",
      "Correct Answer: Red Chamber Art Garden and Jinling Stone Gallery\n",
      "BERT Answer: Red Chamber Art Garden and Jinling Stone Gallery\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: What is in saliva that starts to digest the starches?\n",
      "Question Index: 27344\n",
      "Correct Answer: salivary amylase\n",
      "BERT Answer: salivary amylase\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: Where does the Washington University School of Law rank among \"25 Most Wired Law Schools\"?\n",
      "Question Index: 72519\n",
      "Correct Answer: 4th\n",
      "BERT Answer: None\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: What party in Yugoslavia did he join?\n",
      "Question Index: 17073\n",
      "Correct Answer: Communist Party\n",
      "BERT Answer: Communist\n",
      "10.0\n",
      "####################################################################################\n",
      "Question: Which area contains the Prudhoe Bay Oil Field?\n",
      "Question Index: 60317\n",
      "Correct Answer: The North Slope\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Answer: None\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: How much of the population lives below the poverty line?\n",
      "Question Index: 18790\n",
      "Correct Answer: More than two-thirds\n",
      "BERT Answer: None\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: What two countries have not legally committed to advancing an anti-discriminaory stance towards young people?\n",
      "Question Index: 30380\n",
      "Correct Answer: U.S. and South Sudan\n",
      "BERT Answer: U. S. and South Sudan\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: The Special Clerical Court is accountable to only which body?\n",
      "Question Index: 75821\n",
      "Correct Answer: the Supreme Leader\n",
      "BERT Answer: None\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: Who made the Angel of Independence?\n",
      "Question Index: 39828\n",
      "Correct Answer: the order of the Emperor Maximilian\n",
      "BERT Answer: None\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: Who was the rebel baron leader?\n",
      "Question Index: 28559\n",
      "Correct Answer: Robert fitz Walter\n",
      "BERT Answer: None\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: Why were the Karachays exiled?\n",
      "Question Index: 10342\n",
      "Correct Answer: alleged collaboration with the Germans\n",
      "BERT Answer: their alleged collaboration with the Germans\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: Aside from Switzerland, what country speaks a dialect related to Swiss German?\n",
      "Question Index: 15214\n",
      "Correct Answer: Liechtenstein\n",
      "BERT Answer: Liechtenstein\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: Who guides the students in a formal environment?\n",
      "Question Index: 66547\n",
      "Correct Answer: certified teacher\n",
      "BERT Answer: None\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: What is Seattle's musical genre developed in the 1990s? \n",
      "Question Index: 33035\n",
      "Correct Answer: grunge\n",
      "BERT Answer: grunge\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: What are huge quantities of information stored as?\n",
      "Question Index: 72907\n",
      "Correct Answer: bits\n",
      "BERT Answer: index cards kept in ashoe box\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: How many female Dominican houses were there in Germany?\n",
      "Question Index: 53885\n",
      "Correct Answer: seventy-four\n",
      "BERT Answer: seventy- four\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: Who verbally attacked Steve Stone?\n",
      "Question Index: 37728\n",
      "Correct Answer: Kent Mercker\n",
      "BERT Answer: Kent Mercker\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: When did more madaris begin to form more rapidly?\n",
      "Question Index: 65131\n",
      "Correct Answer: 11th and 12th centuries\n",
      "BERT Answer: Between\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: How many fleet carriers were lost by the Japanese in the Battle of Midway?\n",
      "Question Index: 75062\n",
      "Correct Answer: four\n",
      "BERT Answer: three\n",
      "133.0\n",
      "####################################################################################\n",
      "Question: How big was the tornado in 2011?\n",
      "Question Index: 46963\n",
      "Correct Answer: F3\n",
      "BERT Answer: None\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: What artist made portraits of the Chopin family in 1829?\n",
      "Question Index: 922\n",
      "Correct Answer: Ambrozy Mieroszewski\n",
      "BERT Answer: None\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: Where is the head quarters for the European Space Agency?\n",
      "Question Index: 61669\n",
      "Correct Answer: Paris\n",
      "BERT Answer: Boulogne- Billancourt\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: When do many insectivorous birds migrate?\n",
      "Question Index: 22342\n",
      "Correct Answer: usually at night\n",
      "BERT Answer: usually at night\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: Which two major Protestant denominations are based in Memphis?\n",
      "Question Index: 66798\n",
      "Correct Answer: Church of God in Christ and the Cumberland Presbyterian Church\n",
      "BERT Answer: Church of God in Christ and the Cumberland Presbyterian Church\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: When did the French try to create co-masonic lodges?\n",
      "Question Index: 35451\n",
      "Correct Answer: the 1890s\n",
      "BERT Answer: None\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: What did Darwin learn about about the characteristics of domestic animals?\n",
      "Question Index: 52068\n",
      "Correct Answer: use in our domestic animals strengthens and enlarges certain parts, and disuse diminishes them\n",
      "BERT Answer: developmental and anatomical differences\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: What did submarines sinking Japanese ships do?\n",
      "Question Index: 75088\n",
      "Correct Answer: strangled Japan\n",
      "BERT Answer: strangled Japan\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: What was Clayton Lockett's cause of death?\n",
      "Question Index: 32211\n",
      "Correct Answer: heart attack\n",
      "BERT Answer: cancer\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: In 2013 the number of government jobs was what?\n",
      "Question Index: 11561\n",
      "Correct Answer: 800\n",
      "BERT Answer: None\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: What conference lost the first All-Star Game?\n",
      "Question Index: 14953\n",
      "Correct Answer: American Conference\n",
      "BERT Answer: American Conference\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: Which Austrian political party did Popper join as a youth?\n",
      "Question Index: 60503\n",
      "Correct Answer: Social Democratic Workers' Party of Austria\n",
      "BERT Answer: socialist\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: On what day does The Essentials have its first airing each week?\n",
      "Question Index: 53360\n",
      "Correct Answer: Saturday\n",
      "BERT Answer: June\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: What do Coptic Christians face?\n",
      "Question Index: 80290\n",
      "Correct Answer: discrimination at multiple levels of the government,\n",
      "BERT Answer: discrimination at multiple levels of the government\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: Who is credited with the Vaisesika darsana?\n",
      "Question Index: 53442\n",
      "Correct Answer: Kanada Kasyapa\n",
      "BERT Answer: Pushyamitra Shunga\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: What did Paul want to reform as a newly elected pope?\n",
      "Question Index: 46047\n",
      "Correct Answer: Canon Law\n",
      "BERT Answer: Canon Law\n",
      "2.0\n",
      "####################################################################################\n",
      "Question: How many drugs were approved in 2007?\n",
      "Question Index: 34092\n",
      "Correct Answer: 18 approvals\n",
      "BERT Answer: 2007\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: What president ignored Eisenhower's recommendations in regard to atomic weapons?\n",
      "Question Index: 82843\n",
      "Correct Answer: Truman\n",
      "BERT Answer: None\n",
      "2.0\n",
      "####################################################################################\n",
      "Question: The British revival of Georgian architecture in the 20th century is generally referred to as?\n",
      "Question Index: 76289\n",
      "Correct Answer: Neo-Georgian\n",
      "BERT Answer: Georgian Revival architecture\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: What political theory fascinated Nasser?\n",
      "Question Index: 57184\n",
      "Correct Answer: Egyptian nationalism\n",
      "BERT Answer: Soviet-stylesystem\n",
      "2.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################################################################################\n",
      "Question: Who introduced Irving to Spielberg?\n",
      "Question Index: 68610\n",
      "Correct Answer: Brian De Palma\n",
      "BERT Answer: None\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: Which Chetnik leader did Tito hold talks with?\n",
      "Question Index: 17110\n",
      "Correct Answer: Draza Mihailovic\n",
      "BERT Answer: President Dwight D. Eisenhower\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: How long ago was the decedent of 99% of all modern potatoes cultivated as long ago as?\n",
      "Question Index: 79745\n",
      "Correct Answer: 10,000 years ago\n",
      "BERT Answer: 10, years ago\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: What year was the UAR formed?\n",
      "Question Index: 57370\n",
      "Correct Answer: 1958\n",
      "BERT Answer: 1958\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: When did Elizabeth become the longest reigning monarch?\n",
      "Question Index: 31527\n",
      "Correct Answer: 9 September 2015\n",
      "BERT Answer: 2015\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: What was the estimated population of the whooping crane in 1941?\n",
      "Question Index: 69986\n",
      "Correct Answer: 16 birds\n",
      "BERT Answer: 16 birds\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: When did the Luftwaffe fly inland missions?\n",
      "Question Index: 69886\n",
      "Correct Answer: only on moonlit nights\n",
      "BERT Answer: inland missions\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: What did the working group integrate their ideas with?\n",
      "Question Index: 22833\n",
      "Correct Answer: filter bank\n",
      "BERT Answer: Biocontrol\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: Where are narrative biblical scenes painted in the church at Saint-Savin-sur-Gartempe?\n",
      "Question Index: 21014\n",
      "Correct Answer: on the barrel-vaulted roof\n",
      "BERT Answer: None\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: When does the Premier League have its playing season?\n",
      "Question Index: 74268\n",
      "Correct Answer: During the course of a season (from August to May)\n",
      "BERT Answer: August to May\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: In 1940 what percentage used the Tube for a sleeping shelter?\n",
      "Question Index: 69678\n",
      "Correct Answer: 4%\n",
      "BERT Answer: None\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: What organization did Berners-Lee create?\n",
      "Question Index: 12111\n",
      "Correct Answer: World Wide Web Foundation\n",
      "BERT Answer: World Wide Web Foundation\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: What dialect is the language Romansh similar to?\n",
      "Question Index: 15194\n",
      "Correct Answer: Lombardic alpine\n",
      "BERT Answer: Standard ) German, but the mainspoken language is the Alemannic Swiss German dialect called Bernese German\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: Who held the balance of power in the 1910 election?\n",
      "Question Index: 71350\n",
      "Correct Answer: Labour and Irish Nationalist members.\n",
      "BERT Answer: None\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: What generates protonated molecular hydrogen?\n",
      "Question Index: 11858\n",
      "Correct Answer: ionization of molecular hydrogen from cosmic rays\n",
      "BERT Answer: Electrolysis of brine to yield chlorine\n",
      "3.0\n",
      "####################################################################################\n",
      "Question: What type of sensory issue was a concern prior to the release of 5th gen iPods?\n",
      "Question Index: 1827\n",
      "Correct Answer: hearing loss\n",
      "BERT Answer: weak bass response\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: What was Burke's final publication?\n",
      "Question Index: 45788\n",
      "Correct Answer: Letters on a Regicide Peace\n",
      "BERT Answer: Thoughts and Details on Scarcity\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: How many days did the recount after the 2000 US election last for?\n",
      "Question Index: 80150\n",
      "Correct Answer: 39\n",
      "BERT Answer: 39\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: What would an omnidirectional antenna look like if plotted?\n",
      "Question Index: 71782\n",
      "Correct Answer: donut\n",
      "BERT Answer: non\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: In what borough is the National Tennis Center located?\n",
      "Question Index: 3729\n",
      "Correct Answer: Queens\n",
      "BERT Answer: None\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: Music from what two decades is notably featured on the soft AC format?\n",
      "Question Index: 13796\n",
      "Correct Answer: 1960s and 1970s\n",
      "BERT Answer: 1960s and\n",
      "3.0\n",
      "####################################################################################\n",
      "Question: How many countries got independence from Britain during decolonization?\n",
      "Question Index: 31421\n",
      "Correct Answer: Over 20 countries\n",
      "BERT Answer: None\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: Who coaches the University of Arizona's swim team?\n",
      "Question Index: 73230\n",
      "Correct Answer: Frank Busch\n",
      "BERT Answer: Lute Olson\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: What isotope of uranium was the first to be found fissile?\n",
      "Question Index: 30978\n",
      "Correct Answer: 235\n",
      "BERT Answer: Uranium-\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: What parts of Frederic's personal life influenced his legacy as a leading symbol of the era?\n",
      "Question Index: 807\n",
      "Correct Answer: his love life and his early death\n",
      "BERT Answer: New Spain\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: What group was excluded or persecuted by the empire?\n",
      "Question Index: 81237\n",
      "Correct Answer: Heretics\n",
      "BERT Answer: None\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: Which community is the longest continuous Chinese settlement in the Western World?\n",
      "Question Index: 28237\n",
      "Correct Answer: Melbourne Chinatown\n",
      "BERT Answer: None\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: What modern building material did Viollet-le-Duc teach reform Gothic designers to work with?\n",
      "Question Index: 45196\n",
      "Correct Answer: cast iron\n",
      "BERT Answer: cast iron\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: What are scripts that utilized two seperate cases called?\n",
      "Question Index: 23304\n",
      "Correct Answer: bicameral\n",
      "BERT Answer: bnakavayr\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: What is the name of a magazine published at KU?\n",
      "Question Index: 14486\n",
      "Correct Answer: Jayplay magazine\n",
      "BERT Answer: Jay\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: What did King charles levy on river boats \n",
      "Question Index: 44376\n",
      "Correct Answer: King Charles I, river tolls were levied on boats to pay for the maintenance of the bridge.\n",
      "BERT Answer: \"\n",
      "0.0\n",
      "####################################################################################\n",
      "Question: What part of the V&A was the Royal Institute's Drawing and Archives collection moved to?\n",
      "Question Index: 14234\n",
      "Correct Answer: the Henry Cole Wing\n",
      "BERT Answer: 21 Portman Place to new facilities in the Henry Cole Wing\n",
      "1.0\n",
      "####################################################################################\n",
      "Question: When did the Canadian Armed Forces name cease to be?\n",
      "Question Index: 7913\n",
      "Correct Answer: After the 1980s\n",
      "BERT Answer: None\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# number of questions to test on\n",
    "num_questions = 100\n",
    "\n",
    "# added external loop to save memory so that benchmarking can run faster\n",
    "for _ in range(num_questions):\n",
    "    record_questions = []\n",
    "    record_correct_answer = []\n",
    "    record_answers = []\n",
    "    record_segments = []\n",
    "    record_articles = []\n",
    "\n",
    "    random_indices = list(random.sample(range(0, len(questions_BERT)), 1))\n",
    "\n",
    "    sample_questions = []\n",
    "    for idx in random_indices:\n",
    "        sample_questions.append(questions_BERT[idx])\n",
    "\n",
    "    test_rounds = len(sample_questions)\n",
    "    correct_answers = 0\n",
    "\n",
    "    #define the stopwords\n",
    "    sp = spacy.load('en_core_web_sm')\n",
    "    noncontext_words = sp.Defaults.stop_words\n",
    "   \n",
    "    #loop over the sample questions\n",
    "    for question in sample_questions:\n",
    "        BERT_answers = []\n",
    "        print(\"####################################################################################\")\n",
    "        print('Question:', question)\n",
    "        print('Question Index:', questions_BERT.index(question))\n",
    "        # Dan added dictionary to look up segment\n",
    "        lookup_segment = {}\n",
    "        segments = []\n",
    "        answers_to_consider = []\n",
    "        current = 0\n",
    "\n",
    "        #get the correct answer to this question\n",
    "        correct_answer = question_answers_BERT[questions_BERT.index(question)]\n",
    "        print(\"Correct Answer: %s\" % (correct_answer))\n",
    "        record_correct_answer.append(correct_answer)\n",
    "\n",
    "        #get the top k paragraphs\n",
    "        candidate_articles, article_indices = Retrieve_Article(question,articles_BERT,10)\n",
    "        record_articles.append(candidate_articles)\n",
    "\n",
    "        #segment the chosen candidate article in \"paragraphs\"\n",
    "        candidate_seg_articles = segment_documents(candidate_articles, max_doc_length=450)\n",
    "\n",
    "        #return the answers from each of the top k paragraphs in descending order by relevancy\n",
    "        for seg_idx, segment in enumerate(candidate_seg_articles):\n",
    "            #BERT_prediction, start_idx, end_idx = Run_BERT(question, segment)\n",
    "            BERT_prediction = Run_BERT(question, segment)\n",
    "\n",
    "            BERT_answers.append(BERT_prediction)\n",
    "\n",
    "            ## Dan added this for troubleshooting next part\n",
    "            if '[CLS]' not in BERT_prediction:\n",
    "                # store segment information\n",
    "                lookup_segment[seg_idx] = current\n",
    "                segments.append(segment)\n",
    "\n",
    "                # store answer information extracted from text\n",
    "                answers_to_consider.append(BERT_prediction)\n",
    "\n",
    "\n",
    "                # increment index for dictionary\n",
    "                current +=1\n",
    "            #check to see if the return type is a string\n",
    "            if(type(BERT_prediction)==str):\n",
    "                #create lists of words for the predicted and the correct answers\n",
    "                BERT_pred_list = re.split('\\s+', BERT_prediction)\n",
    "                BERT_true_list = re.split('\\s+', correct_answer)\n",
    "\n",
    "                BERT_pred_list_fix = []\n",
    "                BERT_true_list_fix = []\n",
    "                #remove the stop words in the lists\n",
    "                for word in BERT_pred_list:\n",
    "                    if(word not in noncontext_words):\n",
    "                        BERT_pred_list_fix.append(word)\n",
    "\n",
    "                #remove the stop words in the lists\n",
    "                for word in BERT_true_list:\n",
    "                    if(word not in noncontext_words):\n",
    "                        BERT_true_list_fix.append(word)\n",
    "\n",
    "                #check to see if any words in the prediction are in the answer\n",
    "                true_ans_len = len(BERT_true_list_fix)\n",
    "                num_matches = 0\n",
    "                for word in BERT_pred_list_fix:\n",
    "                    if(word in BERT_true_list_fix):\n",
    "                        num_matches += 1\n",
    "\n",
    "                if(true_ans_len==1):\n",
    "                    if(num_matches==true_ans_len):\n",
    "                        correct_answers += 1\n",
    "                else:\n",
    "                    if(num_matches>=round(0.5*true_ans_len)):\n",
    "                        correct_answers += 1\n",
    "        record_questions.append(question)\n",
    "        record_answers.append(answers_to_consider)\n",
    "        record_segments.append(segments)\n",
    "\n",
    "        final_answer = narrow_down_answers(question, segments, answers_to_consider)   \n",
    "        print('BERT Answer:', final_answer)\n",
    "    BERT_accuracy = correct_answers/test_rounds\n",
    "    print(BERT_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f1a1e9",
   "metadata": {},
   "source": [
    "## BERT Looping Analysis <a id='BERT_analysis'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa1d58b",
   "metadata": {},
   "source": [
    "Check the outputs for a given value in the test set above as well as given refernce information to plug values into the looping portions of the algorithm in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "ff5a0b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correct article is:          Wood\n",
      "Question:                        Are the knots that dead tree limbs form attached or not attached?\n",
      "The correct answer is:           not attached\n",
      "All answers backed out by BERT:  ['not attached']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['composed of wider elements. It is usually lighter in color than that near the outer portion of the ring, and is known as earlywood or springwood. The outer portion formed later in the season is then known as the latewood or summerwood. However, there are major differences, depending on the kind of wood (see below).A knot is a particular type of imperfection in a piece of wood; it will affect the technical properties of the wood, usually reducing the local strength and increasing the tendency for splitting along the wood grain, but may be exploited for visual effect. In a longitudinally sawn plank, a knot will appear as a roughly circular \"solid\" (usually darker) piece of wood around which the grain of the rest of the wood \"flows\" (parts and rejoins). Within a knot, the direction of the wood (grain direction) is up to 90 degrees different from the grain direction of the regular wood.In the tree a knot is either the base of a side branch or a dormant bud. A knot (when the base of a side branch) is conical in shape (hence the roughly circular cross-section) with the inner tip at the point in stem diameter at which the plant\\'s vascular cambium was located when the branch formed as a bud.During the development of a tree, the lower limbs often die, but may remain attached for a time, sometimes years. Subsequent layers of growth of the attaching stem are no longer intimately joined with the dead limb, but are grown around it. Hence, dead branches produce knots which are not attached, and likely to drop out after the tree has been sawn into boards.In grading lumber and structural timber, knots are classified according to their form, size, soundness, and the firmness with which they are held in place. This firmness is affected by, among other factors, the length of time for which the branch was dead while the attaching stem continued to grow.Knots do not necessarily influence the stiffness of structural timber, this will depend on the size and location. Stiffness and elastic strength are more dependent upon the sound wood than upon localized defects. The breaking strength is very susceptible to defects. Sound knots do not weaken wood when subject to compression parallel to the grain.In some decorative applications, wood with knots may be desirable to add visual interest. In applications where wood is painted, such as skirting boards, fascia boards, door frames and furniture, resins present in the timber may continue to \\'bleed\\' through to the surface of a knot for months or even years after manufacture and show as a yellow or brownish stain. A knot primer paint or solution, correctly applied during preparation,']"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = 0\n",
    "true_question_label = question_topic_BERT[questions_BERT.index(record_questions[val])]\n",
    "print('The correct article is:         ', true_question_label)\n",
    "print('Question:                       ', record_questions[val])\n",
    "print('The correct answer is:          ', record_correct_answer[val])\n",
    "print('All answers backed out by BERT: ', record_answers[val])\n",
    "record_segments[val]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a13a90",
   "metadata": {},
   "source": [
    "### Test the Looping Portion to Debug/Understand Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "abf34813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the sentences: [\" Ragworms' jaws are now being studied by engineers as they offer an exceptional combination of lightness and strength.Since annelids are soft-bodied, their fossils are rare - mostly jaws and the mineralized tubes that some of the species secreted. Although some late Ediacaran fossils may represent annelids, the oldest known fossil that is identified with confidence comes from about 518 million years ago in the early Cambrian period.\", ' The 27 surviving panels of the nave are the most important mosaic cycle in Rome of this period. Two other important 5th century mosaics are lost but we know them from 17th-century drawings.', ' The mosaics were executed in the 1220s.Other important Venetian mosaics can be found in the Cathedral of Santa Maria Assunta in Torcello from the 12th century, and in the Basilical of Santi Maria e Donato in Murano with a restored apse mosaic from the 12th century and a beautiful mosaic pavement (1140).', ' The Monastery of Martyrius was founded in the end of the 5th century and it was re-discovered in 1982-85. The most important work of art here is the intact geometric mosaic floor of the refectory although the severely damaged church floor was similarly rich.', ' In addition, it also serves as the seat of Government of Delhi.The foundation stone of the city was laid by George V, Emperor of India during the Delhi Durbar of 1911.', '4 magnitude earthquake in 2015 with its epicentre in Nepal, a 4.7-magnitude earthquake on 25 November 2007, a 4.2-magnitude earthquake on 7 September 2011, a 5.2-magnitude earthquake on 5 March 2012, and a swarm of twelve earthquakes, including four of magnitudes 2.5, 2.8, 3.1, and 3.3, on 12 November 2013.The climate of New Delhi is a monsoon-influenced humid subtropical climate (Koppen Cwa) with high variation between summer and winter in terms of both temperature and rainfall. The temperature varies from 46 degC (115 degF) in summers to around 0 degC (32 degF) in winters.', ' The court also ordered all taxis in the Delhi region to switch to compressed natural gas by March 1, 2016. Transportation vehicles that are more than 10 years old were banned from entering the capital.', ' In addition to the Delhi Metro, a suburban railway, the Delhi Suburban Railway exists.The Delhi Metro is a rapid transit system serving New Delhi, Delhi, Gurgaon, Faridabad, Noida, and Ghaziabad in the National Capital Region of India.', 'Gandhi Smriti in New Delhi is the location where Mahatma Gandhi spent the last 144 days of his life and was assassinated on 30 January 1948. Rajghat is the place where Mahatma Gandhi was cremated on 31 January 1948 after his assassination and his ashes were buried and make it a final resting place beside the sanctity of the Yamuna River.', \" Government and quasi government sector was the primary employer in New Delhi. The city's service sector has expanded due in part to the large skilled English-speaking workforce that has attracted many multinational companies. Key service industries include information technology, telecommunications, hotels, banking, media and tourism.\"]\n",
      "Final answer: an\n"
     ]
    }
   ],
   "source": [
    "print('Final answer:', narrow_down_answers(record_questions[val], record_segments[val], record_answers[val]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d73b14",
   "metadata": {},
   "source": [
    "### Print the Beginning of the Top K Articles for Checking Article Retrieval Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "78f2ef54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main passenger airport serving the metropolis and the state is Melbourne Airport (also called Tullamarine Airport), which is the second busiest in Australia, and the Port of Melbourne is Australia\n",
      "\n",
      "Raleigh (/'ra:li/; RAH-lee) is the capital of the state of North Carolina as well as the seat of Wake County in the United States. It is the second most populous city in North Carolina, after Charlott\n",
      "\n",
      "The University of Kansas (KU) is a public research university and the largest in the U.S. state of Kansas. KU branch campuses are located in the towns of Lawrence, Wichita, Overland Park, Salina, and \n",
      "\n",
      "An exhibition game (also known as a friendly, a scrimmage, a demonstration, a preseason game, a warmup match, or a preparation match, depending at least in part on the sport) is a sporting event whose\n",
      "\n",
      "Victoria married her first cousin, Prince Albert of Saxe-Coburg and Gotha, in 1840. Their nine children married into royal and noble families across the continent, tying them together and earning her \n",
      "\n",
      "Insects (from Latin insectum, a calque of Greek entomon [entomon], \"cut into sections\") are a class of invertebrates within the arthropod phylum that have a chitinous exoskeleton, a three-part body (h\n",
      "\n",
      "Everton were founder members of the Premier League in 1992, but struggled to find the right manager. Howard Kendall had returned in 1990 but could not repeat his previous success, while his successor,\n",
      "\n",
      "Oklahoma i/,oUkl@'hoUm@/ (Cherokee: Asgaya gigageyi / asgaya gigageyi; or translated ogalahoma (ogalahoma), Pawnee: Uukuhuuwa, Cayuga: Gahnawiyo?geh) is a state located in the South Central United Sta\n",
      "\n",
      "Armenia is a unitary, multi-party, democratic nation-state with an ancient cultural heritage. Urartu was established in 860 BC and by the 6th century BC it was replaced by the Satrapy of Armenia. In t\n",
      "\n",
      "A transistor is a semiconductor device used to amplify or switch electronic signals and electrical power. It is composed of semiconductor material with at least three terminals for connection to an ex\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for q in record_articles[val]:\n",
    "    print(q[0:200], end='\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
